# 估计数据X与y的关系
本节中给定数据X与样本标签d，请估计X与d之间的关系。在评估过程中建议按照如下过程进行
- 读取：数据格式为npz文件
- 预处理：数据中是否存在异常数据，并进行排除
- 观察：观察给定数据X与d的形式，并进行绘图
- 建模：通过观察，给定函数y=f(x)的形式
- 求解：求解建模过程中的函数参数
- 预测：预测某点预期输出值


# 参考思路
## 1. 读取并绘图
```python
import numpy as np
# 读取数据
file = np.load("homework.npz")
X = file['X']
d = file['d']
# 观察输入矩阵形式
print("X Shape:{}; d Shape:{}".format(np.shape(X), np.shape(d)))
# 绘图
import matplotlib.pyplot as plt
plt.scatter(X[:, 0], d[:, 0])
plt.show()
```

## 2. 通过观察给定数据形式
通过观察散点图可以假设两列数据关系为线性关系，所以进行如下建模过程：

- 假定数据形式为：y=ax+b
- 数据为连续型分布，所以选择二范数来衡量y与d之间相似程度：loss=(y-d)^2

参考代码：
```python
def f(x, w):
    a, b = w
    return a * x + b
```


## 3. 求解
求解过程就是求可训练参数偏导（梯度）的过程：

$$
\begin{matrix}
\frac{\partial loss}{\partial a}=2(y-d)x\\
\frac{\partial loss}{\partial b}=2(y-d)
\end{matrix}
$$

参考代码：
```python
def grad_f(x, d, w):
    a, b = w
    y = f(x, w)
    grad_a = 2 * (y - d) * x
    grad_b = 2 * (y - d)
    return grad_a, grad_b
```

### 3.1 两种迭代选择方式：
1. 一次输出一个样本：计算单个可训练参数的梯度，并进行迭代
2. 一次输入多个样本：计算多个可训练参数的梯度取平均，并进行迭代

第一种方式参考代码：
```python
w = [0, 0]
eta = 0.1
for itr in range(1000):
    idx = np.random.randint(0, len(X))
    inx = X[idx]
    ind = d[idx]
    ga, gb = grad_f(inx, ind, w)
    w[0] -= eta * ga
    w[1] -= eta * gb
```
第二种方式参考代码：
```python
w = [0, 0]
eta = 0.1
batchsize=10
for itr in range(100):
    sum_ga, sum_gb = 0, 0
    for _ in range(batchsize):
        idx = np.random.randint(0, len(X))
        inx = X[idx]
        ind = d[idx]
        ga, gb = grad_f(inx, ind, w)
        sum_ga += ga
        sum_ga += gb
    w[0] -= eta * sum_ga / batchsize
    w[1] -= eta * sum_gb / batchsize
```

## 4. 预测
给定数据点，预测其输出值，并进行绘图：

```python
x = np.linspace(-2, 4, 100)
y = f(x, w)
plt.scatter(X[:, 0], d[:, 0])
plt.plot(x, y)
plt.show()
```


## 5. 完整参考代码注释

```python
import numpy as np
file = np.load("homework.npz")
X = file['X']
d = file['d']

# 定义函数，x为输入，w为可训练参数
def f(x, w):
    a, b = w
    return a * x + b
# 定义函数关于可训练参数的偏导数
def grad_f(x, d, w):
    a, b = w
    y = f(x, w)
    grad_a = 2 * (y - d) * x
    grad_b = 2 * (y - d)
    return grad_a, grad_b
# 定义初始值
w = [0, 0]
eta = 0.1
################################################################
# 第一种迭代方式
################################################################
for itr in range(1000):
    idx = np.random.randint(0, len(X))
    inx = X[idx]
    ind = d[idx]
    ga, gb = grad_f(inx, ind, w)
    w[0] -= eta * ga
    w[1] -= eta * gb

# 定义初始值
w = [0, 0]
eta = 0.1
batchsize=10
#################################################################
# 第二种迭代方式
#################################################################
for itr in range(100):
    sum_ga, sum_gb = 0, 0
    for _ in range(batchsize):
        idx = np.random.randint(0, len(X))
        inx = X[idx]
        ind = d[idx]
        ga, gb = grad_f(inx, ind, w)
        sum_ga += ga
        sum_ga += gb
    sum_ga = sum_ga / batchsize
    sum_gb = sum_gb / batchsize
    w[0] -= eta * sum_ga
    w[1] -= eta * sum_gb

# 预测过程
import matplotlib.pyplot as plt
import matplotlib
# 引入中文字库
matplotlib.rcParams['font.sans-serif'] = 'SimHei'
# 输入x
x = np.linspace(-2, 4, 100)
# 输出y
y = f(x, w)
# 绘图
plt.scatter(X[:, 0], d[:, 0], s=20, alpha=0.4, label="数据散点")
plt.plot(x, y, lw=5, color="#990000", alpha=0.5, label="预测关系")
plt.legend()
plt.show()
```

## 还需要做什么？
- 是否有更好的建模方式
- 如何评价更好？

### 附加：另一种建模方式
使得曲线变得更加复杂，在传统线性的基础上，加入非线性函数:
y=f(ax+b)
那么此时与前面的代码相比实际上添加了一个：
$$
\begin{matrix}
\frac{\partial loss}{\partial a}=2(y-d)f'(ax+b)x\\
\frac{\partial loss}{\partial b}=2(y-d)f'(ax+b)
\end{matrix}
$$


```python
# 定义非线性函数
def func(x):
    ret = np.array(x)
    ret[x<0] = 0
    return ret
def dfunc(x):
    ret = np.zeros_like(x)
    ret[x>0] = 1
    return ret
def f(x, w):
    a, b = w
    return func(a * x + b)
# 定义函数关于可训练参数的偏导数
def grad_f(x, d, w):
    a, b = w
    y = f(x, w)
    dy = dfunc(a * x + b)
    grad_a = 2 * (y - d) * dy * x
    grad_b = 2 * (y - d) * dy
    return grad_a, grad_b
```


